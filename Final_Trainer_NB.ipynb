{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader as pdr\n",
    "from utils import *\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization,Bidirectional,TimeDistributed,Dense,Input,Conv2D,MaxPool1D,Activation,Dropout,Flatten,Conv1D,concatenate,Embedding,LSTM\n",
    "# from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "import tensorflow as tf\n",
    "from sklearn.externals import joblib \n",
    "from tensorflow.keras.initializers import he_normal,glorot_normal,he_uniform\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_companies = ['AMD','INTC','NVDA','MSFT','QCOM','TSM','SPY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of companies that are realted to AMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology:\n",
    "    1) The training will be done to predict each company stock price for each loop\n",
    "    \n",
    "        Ex: Loop 1:\n",
    "            Target Varaible : 'AMD': AMD\n",
    "            Data : 'INTC' : Intel ,'NVDA': Nvidia,'MSFT' : Microsoft,'QCOM': Qualcom,'TSM': TSMC,'SPY' : SPDR S&P 500 Trust ETF\n",
    "            Loop 2:\n",
    "            Target Varaible : 'INTC' : Intel\n",
    "            Data :  'AMD': AMD,'NVDA': Nvidia,'MSFT' : Microsoft,'QCOM': Qualcom,'TSM': TSMC,'SPY' : SPDR S&P 500 Trust ETF\n",
    "        And so on\n",
    "    2) We have considered training data only till May 31st 2020.\n",
    "       Test data from Jan 1st to May 31 2020.\n",
    "        But have not given June 2020 as input instead we run the model and get June 1st 2020 data for each company \n",
    "        and then \n",
    "        we predict June 2nd 2020 data.\n",
    "        \n",
    "        This way we can run the model to simulate the future inputs to the model and predict the future when the data is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(list_of_companies):\n",
    "    for company in list_of_companies:\n",
    "        # Fetching data from Jan 1st 2010 to Dec 31st 2020\n",
    "        # Please note that we will training a seperate model for each company\n",
    "        for stock_name in list_of_companies:\n",
    "            if not os.path.isfile(\"Data_train/\"+ stock_name + '.csv'):\n",
    "                df = pdr.get_data_tiingo(symbols=[stock_name],start='1/1/2010',end = '05/31/2020', api_key='bc41cd89fc117b78bbde26797c2f1b72b7aa83c2')\n",
    "                df.to_csv( \"Data_train/\"+ stock_name + '.csv')\n",
    "                df = pd.read_csv(\"Data_train/\"+ stock_name + '.csv')\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df['date'] = df['date'].apply(lambda x: x.date())\n",
    "                df.to_csv( \"Data_train/\"+ stock_name + '.csv')\n",
    "            elif stock_name == company:\n",
    "                company_df = pd.read_csv('Data_train/' + company + '.csv')\n",
    "            else:\n",
    "                pass\n",
    "        # Reading the data for target company\n",
    "        company_df = pd.read_csv('Data_train/' + company + '.csv',index_col=0)\n",
    "        \n",
    "        company_df = company_df[['date','close']]\n",
    "        def technical_indicators(df):\n",
    "            # Moving Average\n",
    "            df['week_moving_avg'] = df['close'].rolling(window=7,min_periods=1).mean()\n",
    "            df['21d_moving_avg'] = df['close'].rolling(window=21,min_periods=1).mean()\n",
    "\n",
    "            # MACD\n",
    "            df['12_ema'] = df[['close']].ewm(span=12).mean()\n",
    "            df['26_ema'] = df[['close']].ewm(span=26).mean()\n",
    "            df['macd'] = df['12_ema'] - df['26_ema']\n",
    "\n",
    "            # Bollinger Bands\n",
    "            df['20_day_std'] = df['close'].rolling(window=20,min_periods=0).std()\n",
    "            df['upper_band'] = df['21d_moving_avg'] + (df['20_day_std']*2)\n",
    "            df['lower_band'] = df['21d_moving_avg'] - (df['20_day_std']*2)\n",
    "\n",
    "            return df\n",
    "        # Adding technical_indicators for the dataset\n",
    "        company_df = technical_indicators(company_df)\n",
    "                \n",
    "        # Adding forier transform feature to the data\n",
    "        close_fft = np.fft.fft(np.asarray(company_df['close'].tolist()))\n",
    "        fft_df = pd.DataFrame({'fft':close_fft})\n",
    "        fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "        fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "        \n",
    "        \n",
    "        company_df = pd.concat([company_df,fft_df],axis=1)\n",
    "        \n",
    "        company_df = company_df.drop(['date'],axis=1)\n",
    "        company_df = company_df.drop(['fft'],axis=1)\n",
    "        \n",
    "        # Adding other related companies closing period for the data\n",
    "        for others in list_of_companies:\n",
    "            if others == company:\n",
    "                pass\n",
    "            else:\n",
    "                temp_df = pd.read_csv(\"Data_train/\"+ others + '.csv',index_col=0)\n",
    "                company_df[others] = temp_df['close']\n",
    "                \n",
    "        \n",
    "        company_df['target'] = company_df['close']\n",
    "        company_df = company_df.drop(['close'],axis=1)\n",
    "        \n",
    "        # Defining scaling function\n",
    "        scale = MinMaxScaler()\n",
    "        \n",
    "        company_df_scale = scale.fit_transform(company_df)\n",
    "        company_df_scale =  pd.DataFrame(company_df_scale,columns=company_df.columns)\n",
    "        # Saving scalar for production purposes\n",
    "        joblib.dump(scale,'min_max_scaler_' + company + '.pkl')\n",
    "        \n",
    "        # This function will convert the data into T-20 : T+70 per row\n",
    "        def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "            n_vars = 1 if type(data) is list else data.shape[1]\n",
    "            df = pd.DataFrame(data)\n",
    "            cols, names = list(), list()\n",
    "            # input sequence (t-n, ... t-1)\n",
    "            for i in range(n_in, 0, -1):\n",
    "                cols.append(df.shift(i))\n",
    "                names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            # forecast sequence (t, t+1, ... t+n)\n",
    "            for i in range(0, n_out):\n",
    "                cols.append(df.shift(-i))\n",
    "                if i == 0:\n",
    "                    names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "                else:\n",
    "                    names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            # put it all together\n",
    "            agg = pd.concat(cols, axis=1)\n",
    "            agg.columns = names\n",
    "            # drop rows with NaN values\n",
    "            if dropnan:\n",
    "                agg.dropna(inplace=True)\n",
    "            return agg\n",
    "        \n",
    "        print(company_df_scale.shape)\n",
    "        time_series_data = series_to_supervised(company_df_scale, 20,50)\n",
    "        # Model Parameters\n",
    "        past_time = 70\n",
    "        no_of_features = 17\n",
    "        observation_set = past_time * no_of_features\n",
    "        print(time_series_data.shape)\n",
    "        train_X, train_y = time_series_data.values[:, :observation_set], time_series_data.values[:, -1]\n",
    "        \n",
    "        train_X = train_X.reshape(train_X.shape[0], past_time,no_of_features)\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # Model \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(500, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam',metrics=['mse'])\n",
    "        \n",
    "        # Training model for stock price given few other related companies data\n",
    "        model.fit(x=train_X,y=train_y,epochs=10,verbose=0)\n",
    "        \n",
    "        # Saving the model for production purposes\n",
    "        model.save('time_series_model_' + company + '.h5')\n",
    "        \n",
    "        print(company + ' model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "AMD model saved\n",
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "INTC model saved\n",
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "NVDA model saved\n",
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "MSFT model saved\n",
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "QCOM model saved\n",
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "TSM model saved\n",
      "(2619, 17)\n",
      "(2549, 1190)\n",
      "SPY model saved\n"
     ]
    }
   ],
   "source": [
    "model_trainer(related_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
